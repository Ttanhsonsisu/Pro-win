{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Digit Recognizer**\n",
    "This is a idk, like a guide on how to make a nn model that can read doctor's writing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparing the data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Numpy*: for creation of arrays and bring me back the good ol' day of linear algebra\n",
    "- *Pandas*: for file reading (mainly)\n",
    "- *Mathplotlib*: for the training process visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data from the csv file using *pd.read_csv()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/CODEEEEE/NOTEBOOK/artificial_retard/digit_recognizer/mnist_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *data.head()* to like see some funny spreadsheet and stuff :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn it into an array, then shuffle the fuck out of it as if you were cooking sum of data gud spicy hot noddle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "#shuffle da data :D\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into 3 sets:\n",
    "- *Train set*: to **form** the neural network\n",
    "- *Validation(dev) set*: to **evaluate** and **tweak** the **hyperparameters**\n",
    "- *Test set*: to force the model to attend the **university entrance exam** for a chance of getting a **degree** to finally end up in a **job at mc donald**, working **minimum wages**, can't afford **accommodation** for family which inevitably result in **divorce** (you bet your ass that model gonna lose the right for its children in court cuz its vocabulary is limited to only 10 numbers) (its wife is the letter recognizer, the ability to actually construct a meaningful sentence proven to be quite an important asset in convincing someone)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial data that we were give has a shape of **({i forgor}, 785)**, or in dummy term: an array with **{i forgor}** values (we shall call each of those value **dat[i]**)\n",
    "\n",
    "Each of **dat[i]** has 785 values:\n",
    "- *1st one* being **label** (the answer)\n",
    "- *The 784 rest* being the **brightness** of each pixel in the handwriting\n",
    "\n",
    "The idea here is to **seperate** between the **labels** and the **pixel values** and put them into corresponding set:\n",
    "- *X*: representation of the handwriting [ [hwrite], ..., [hwrite] ]\n",
    "- *Y*: the label, or correct answer [ ans, ..., ans ]\n",
    "\n",
    "You can think of the initial data as a **matrix** with **{i forgor} rows, 785 columns**. The initial idea would be to create a **for loop** incrementing through each **dat[i]** and throw them into a placeholder array, but that is going to take like a bocchilion year\n",
    "\n",
    "Here's what you can do, from a linear algebra standpoint:\n",
    "- *Transpose the data to get Ys*: doing so will cause the labels to all be put in **one row** which you can extract into your **Ys**\n",
    "- *Retranspose the data to fix Xs*: Xs on the first transpose has been tainted, so we can just **rotate** it again to untainted it\n",
    "- *ACTUALLY, don't retranspose the data to fix Xs*: because of the nature of my code, keeping it transposed is going to help with the **dot product** in the upcoming code **(forward prop)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "data_train = data[0:50000].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255\n",
    "_, m_train = X_train.shape\n",
    "\n",
    "#validation set\n",
    "data_dev = data[50000:55000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255\n",
    "\n",
    "#test set\n",
    "data_test = data[55000: m].T\n",
    "Y_test = data_test[0]\n",
    "X_test = data_test[1:n]\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Learning Process**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions will be used to determined the brightness of each neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def deriv_ReLU(x): #get derivative\n",
    "    return x > 0\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / sum(np.exp(x))\n",
    "\n",
    "def one_hot(y):\n",
    "    one_hot_y = np.zeros( (y.size, y.max() + 1) )\n",
    "    one_hot_y[np.arange(y.size), y] = 1\n",
    "    one_hot_y = one_hot_y.T\n",
    "    return one_hot_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each parameter is an array of many shapes, we use some linear algebra magic (*dot product*) to make every thing fit nicely together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    w1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    w2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d = init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.93069715, 2.30476947, 5.66186462, ..., 3.80135777, 0.33769165,\n",
       "       3.50870718])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dot(X_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train START!!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nn train through 3 processes:\n",
    "- *Forward Pass*: throw the weights and biases thru the nn to determine the prediction\n",
    "- *Backprobagation*: using the chain rule of calculus (fuck) and gradient descent to determine the changes in the weights and biases\n",
    "- *Update Parameters*: change the w, b, that's it lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(w1, b1, w2, b2, x):\n",
    "    z1 = w1.dot(x) + b1 # z = weight*x + bias\n",
    "    a1 = ReLU(z1) # activation = relu(z)\n",
    "    z2 = w2.dot(a1) + b2\n",
    "    a2 = softmax(z2) # output\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "def backward_prop(z1, a1, z2, a2, w1, w2, x, y):\n",
    "    m = y.size\n",
    "    one_hot_y = one_hot(y)\n",
    "\n",
    "    dz2 = a2 - one_hot_y\n",
    "    dw2 = 1 / m * dz2.dot(a1.T)\n",
    "    db2 = 1 / m * np.sum(dz2)\n",
    "\n",
    "    dz1 = w2.T.dot(dz2) * deriv_ReLU(z1)\n",
    "    dw1 = 1 / m * dz2.dot(x.T)\n",
    "    db1 = 1 / m * np.sum(dz1)\n",
    "\n",
    "    return dw1, db1, dw2, db2\n",
    "\n",
    "def update_params(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha):\n",
    "    w1 -= dw1 * alpha\n",
    "    b1 -= db1 * alpha\n",
    "    w2 -= dw2 * alpha\n",
    "    b2 -= db2 * alpha\n",
    "    return w1, b1, w2, b2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[**!**] Try to set the hyperparameters of the grad_descent() function so that the model don't underfit (literally an idiot) or overfit (memorization onetrick)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad descent\n",
    "def get_prediction(x):\n",
    "    return np.argmax(x, 0)\n",
    "\n",
    "def get_accuracy(predict, y):\n",
    "    #this get the sum of all correct / sum of all\n",
    "    return np.sum(predict == y) / y.size\n",
    "\n",
    "def grad_descent(x, y, alpha=0.1, iteration=500, iter_step=50):\n",
    "    w1, b1, w2, b2 = init_params()\n",
    "\n",
    "    for i in range(iteration):\n",
    "        z1, a1, z2, a2 = forward_prop(w1, b1, w2, b2, x)\n",
    "        dw1, db1, dw2, db2 = backward_prop(z1, a1, z2, a2, w1, w2, x, y)\n",
    "        w1, b1, w2, b2 = update_params(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha)\n",
    "\n",
    "        if i % iter_step == 0:\n",
    "            print('Iteration: ', i, ' -> ', i + iter_step)\n",
    "            print('Acc:', get_accuracy( get_prediction(a2), y ) * 100)\n",
    "    \n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tiny piece of code here is just to start the training process (through the for loop). After finishing, these 4 coolass mfs will be the final product and going to be use for every future prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1, w2, b2 = grad_descent(X_train, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Graduation Day :D**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for testing, there are two mode:\n",
    "- Testing from the *test set*\n",
    "- Testing from your *own data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict(X, W1, b1, W2, b2):\n",
    "    _, _, _, a2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    return get_prediction(a2)\n",
    "\n",
    "def test_prediction(w1, b1, w2, b2, img_data=None, img_label=None):\n",
    "    if img_data is None:\n",
    "        return\n",
    "\n",
    "    current_image = img_data\n",
    "\n",
    "    prediction = make_predict(current_image, w1, b1, w2, b2)\n",
    "    print(\"Prediction: \", prediction[0])\n",
    "\n",
    "    if img_label is not None:\n",
    "        print(\"Label: \", img_label)\n",
    "\n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "def predict_from_testset(w1, b1, w2, b2, index):\n",
    "    test_prediction(w1, b1, w2, b2, X_test[:, index, None], Y_test[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_testset(w1, b1, w2, b2, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your own input\n",
    "input_file = open('/CODEEEEE/NOTEBOOK/artificial_retard/digit_recognizer/test.txt')\n",
    "\n",
    "conv = np.loadtxt(input_file, delimiter=',', unpack=False).reshape((-1, 1))\n",
    "\n",
    "test_prediction(w1, b1, w2, b2, conv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
